\section{Experiments}


In this section, we first evaluate the proposed CMF module on two tasks: text-video retrieval and very-large scale action recognition. Then we present an ablation study to explore how the performance of the proposed approach is affected by different CMF designs. 

% In this section, We first provide the description of datasets, implementation details and evaluation metric used in two tasks. Then We evaluate the proposed CMF approach on four benchmarks for video retrieval and a large scale action recognition benchmark. Finally we present an ablation study to explore how the performance of the proposed approach is affected by different CMF designs. 



\subsection{Video Retrieval}

\subsubsection{Dataset and Implementation Details}

We perform experiments on four video datasets:  MSR-VTT~\cite{xu2016msr}, LSMDC~\cite{rohrbach2015dataset}, DiDeMo~\cite{anne2017localizing} and ActivityNet-captions~\cite{krishna2017dense}, covering a challenging set of domains which include videos from YouTube, personal collections and movies. 

We begin by evaluating the module on standard text-video retrieval benchmarks.  \samsays{TODO: Add detailed dataset descriptions and evaluation protocols}


\paragraph{ActivityNet-captions:} \cite{krishna2017dense} comprises a collection of 20k videos sourced from YouTube.  Each video is accompanied by a descriptive sentences, numbering 100k in total.  For fair comparison, we adopt the retrieval evaluations and protocols used in prior work \cite{zhang2018cross,liu2019use}, training up to 15 epochs on the training partition (10,009 videos in total) and evaluating on the larger \texttt{val1} (4,917 videos in total).  We report performance in Tab.~\ref{table:activity-net}.


\input{tables/activity-net.tex}

\paragraph{DiDeMo:} \cite{anne2017localizing} consists of a set of personal videos, captured in a diverse array of visual settings (10,464 videos in total).  We follow prior work on \textit{video-level} text-video retrieval (we do not make use of timestamp information during training or testing).  We compare to competing methods in Tab.~\ref{table:DiDeMo}.  \samsays{We may need to report numbers on the DiDeMo val set for a fair comparison with CE.}


\input{tables/didemo.tex}

\paragraph{MSR-VTT:} \cite{xu2016msr} We report results in Tab.~\ref{table:MSRVTT}. 

\input{tables/msrvtt.tex}

\paragraph{LSMDC:} \cite{rohrbach2015dataset}.  We report results in Tab.~\ref{table:LSMDC-MSVD} (left)

\input{tables/lsmdc-msvd.tex}

\paragraph{MSVD:} \cite{chen2011collecting}  We report results in Tab.~\ref{table:LSMDC-MSVD} (right)

\subsection{Action Recognition}

\paragraph{Multi-Moments-in-Time:} This dataset is a recently released very-large scale collection of short videos for action recognition.  The Multi-Moments-in-Time dataset contains 313 classes, 1025862 training videos and 10000 validation videos \cite{monfortmoments}.

\subsection{Ablation Study: Module Design}

In this section, we assess the contribution of the proposed HOI module.  To do so, we conduct further experiments on the MSR-VTT dataset as well as the Mult-Moments-in-Time dataset.
\input{tables/ablation-tab.tex}
\subsection{Ablation Study: Temporal Aggregation}

\subsection{Computation Burden Analysis}