\begin{abstract}
% In this work, we investigate the Cross Modal Filter, a technique which aims to exploit signal in one modality to resolve ambiguities in another. 
The objective of this work is video representation---we seek to compress the high-dimensional, dynamic content of a video into a compact, fixed-size vector that enables efficient retrieval and content classification.  The key idea underpinning our approach is \textit{Cross-Modal Enhancement} (CME), the technique of exploiting the context provided by signals in one modality to resolve ambiguities within another when forming the video embedding.  We investigate several instantiations of this core idea, charting the design space of different cross-modal enhancement mechanisms which trade-off computational complexity against performance.  We demonstrate the general applicability of cross-modal enhancement on different video understanding tasks: text-video retrieval and very large-scale action recognition. In the retrieval setting, we establish state-of-the-art performances on four standard benchmarks: ActivityNet, MSR-VTT, LSMDC and DiDeMo.  We then further establish the effectiveness of our approach for the task of video classification on the recently proposed Multi-Moments-in-Time benchmark.  
\end{abstract}   